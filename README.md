# Urdu Poetry Generation

This project explores **Urdu poetry text generation** using
state-of-the-art sequence models. The goal is to generate coherent,
stylistically rich, and meaningful Urdu poetry using RNN, LSTM, and
Transformer architectures, trained on a curated classical poetry
dataset.

The project includes **quantitative evaluation (loss, perplexity,
accuracy)** and **qualitative evaluation (flow, rhyme, human review)**
to compare model performance across architectures and optimizers.

## ğŸ“š Dataset

**Source:** HuggingFace -- `ReySajju742/Urdu-Poetry-Dataset`\
**Total Poems:** 1,323\
**Content:** Classical Urdu poetry (Ghalib, Iqbal, Mir, etc.)\
**Size:** 1.38 MB

### Preprocessing Steps

-   Extract individual lines from poems\
-   Character-level tokenization using Keras Tokenizer\
-   Build vocabulary\
-   Create n-gram sequences for next-character prediction\
-   Pad sequences to uniform length\
-   Train/Val/Test split: **80% / 10% / 10%**

## ğŸ§  Models Implemented

### **1. Simple RNN**

-   Baseline sequential model\
-   Fast but struggles with long-term dependencies

### **2. LSTM (Long Short-Term Memory)**

-   Captures longer dependencies\
-   Best overall performance in this project

### **3. Transformer**

-   Self-attention architecture\
-   High training cost\
-   Competitive perplexity on tuned versions

## âš™ Optimization Algorithms

-   **Adam**
-   **RMSprop**
-   **SGD with Momentum**

## ğŸ— Training Configuration

-   **Epochs:** 20--30 (Early Stopping enabled)\
-   **Batch Size:** 128\
-   **Early Stopping Patience:** 5\
-   **Hardware:** GPU recommended (LSTM/Transformer especially)

## ğŸ“Š Evaluation Metrics

### Quantitative

-   Train & Validation Loss\
-   Test Perplexity\
-   Accuracy (character prediction)\
-   Training Time (minutes)

### Qualitative

-   Rhyme quality\
-   Meter consistency\
-   Stylistic coherence\
-   Human evaluation

## ğŸ“ˆ Model Performance Summary

### **Perplexity Comparison (Test Set)**

  Model                         Optimizer   Test Loss    Perplexity   Training Time (mins)
  ----------------------------- ----------- ------------ ------------ ----------------------
  RNN                           Adam        1.6948       5.445        300
  RNN                           RMSprop     1.703        5.491        300
  RNN                           SGD         1.6898       5.418        300
  RNN (2-Layer + Dropout 0.2)   Adam        1.6798       5.36         1200
  LSTM                          Adam        1.5854       4.881        2400
  LSTM                          RMSprop     1.6406       5.158        1800
  LSTM                          SGD         1.5899       4.904        3420
  LSTM (3-Layer)                Adam        **1.5563**   **4.742**    4800
  Transformer (Set1)            Adam        1.7194       5.581        6000
  Transformer (Set2)            Adam        1.6902       5.420        10800

## ğŸ¨ Visualizations

All plots are available in the `visualizations/` folder:

-   **perplexity_comparison.png**
-   **training_time_comparison.png**
-   **perplexity_heatmap.png**

## âœ Sample Generated Poetry

### Seed: **"Ø¯Ù„ Ú©ÛŒ Ø¨Ø§Øª"**

#### **RNN (Adam)**

Ø¯Ù„ Ú©ÛŒ Ø¨Ø§Øª Ú†Ù„ Ù„ÛŒØ§ Ø³ÛŒ Ù¾Ú¾Ø± Ø¨Ø§Ù‚ÛŒ Ú©ÛŒ Ú©Ø§Ø´ Ù…ÛŒÚº Ú©ÙˆØ¦ÛŒ ÛŒÛ Ø±ÙˆØ­ ÛŒØ§ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ÛŒ Ø³Ù† Ø¢Ù…Ø¯
Ú©ÛŒØ§ Ú†ÛŒØ² Ù…ÛŒÚº Ù¾Ú¾Ø± ÛŒÛ ØªÙˆ ØªÙˆ Ù…ÛŒÚº Ù†Û ÛÙˆØ§ Ú©Ø± Ù¾Ú¾Ø± Ø¨Ù†Ø§ Ù„ÛÙˆ Ø¯Ú©Ú¾ Ø¨Ú¾ÛŒ Ø¯Ø¹Ø§ Ø¯Ø§Ù… Ù…ÛŒÚº
Ù¾Ø±Ø¯ÛÙ” ØºÙÙ„Û Ùˆ Ø³ÙˆØ§ Ø¯Ù„ Ø³Û’ Ù…Ø±Ø­Ù…Ù†Ø¯Û‚ Ø±Ú©Ú¾ØªØ§ Ø§Ø³ Ø´ÛŒØ´Û Ø³Û’ Ø¨Û’ Ø­Ø§Øª Ø¨Ú¾Ù„Ø§ Ú¯ÛŒØ§ Ø¨Ú¾

#### **LSTM (Adam)**
Ù…Ø­Ø¨Øª Ù†ÛÛŒÚº ÛÙˆØ¦ÛŒ Ú©Û Ø¢Ø¦ÛŒ ÛÛ’ Ù…ÛØ±Ø¨Ø§Úº Ú©ÛŒÙˆÚº ÛÛ’ Ú©Û Ø§Ø³
Ú©Ùˆ Ù…Ù„Ù†Ø§ Ù†Û ÛŒÛ ØªÙˆ Ù…Ø±Û’ Ø¨Ø¹Ø¯ Ù…Ù†ØµÙÛŒ Ø³Û’ ÛÙ… Ù†Û’ ØªÙˆ Ø§Ù†
ØªÛŒØ² ÛÛ’ Ø§Ù†ØªØ¸Ø§Ø± Ù†ÛÛŒÚº ÛÙˆØªÛŒ Ø§Ù† Ú©ÛŒ Ø¢Ù†Ú©Ú¾ÙˆÚº Ù…ÛŒÚº Ú©Ú†Ú¾
Ú©ÙˆØ¦ÛŒ Ú©ÛØ§Ù†ÛŒ ØªÚ¾ÛŒ ÙˆÛ Ø¨Ú¾ÛŒ Ø³Ù…Ø¬Ú¾ØªÛ’ ØªÚ¾Û’ Ù¾Ú¾Ø± ØªÙˆ Ø¯ÛŒÚ©Ú¾Ùˆ
ØªÙˆ ØºØ§Ù„Ø¨Ø” Ù…ÛŒÚº Ú©Ø³ÛŒ Ø³Û’ Ú©
#### **Transformer (Adam)**

Ø¯Ù„ Ú©ÛŒ Ø¨Ø§Øª ÙˆÛ Ø³Ø¨ Ú©Ú†Ú¾ Ú©ÛØªØ§ ÛÛ’ØŒ Ù„ÛŒÚ©Ù† ÙˆÙ‚Øª Ú©ÛŒ Ø±ÛÙ†Ù…Ø§Ø¦ÛŒ Ù…ÛŒÚº ÛÙ… Ú©ÛÛŒÚº Ú©Ú¾Ùˆ Ø¬Ø§ØªÛ’
ÛÛŒÚºØŒ\
Ø§ÙˆØ± ÛØ± Ù„Ù…Ø­Û Ø¬Ùˆ Ú¯Ø²Ø±ØªØ§ ÛÛ’ØŒ Ù†Ø¦Û’ Ø§Ø´Ø¹Ø§Ø± Ú©ÛŒ Ø´Ú©Ù„ Ø§Ø®ØªÛŒØ§Ø± Ú©Ø± Ù„ÛŒØªØ§ ÛÛ’

## ğŸ“ Folder Structure

    urdu-poetry-project/
    â”‚
    â”œâ”€â”€ data/                # Raw and processed data
    â”œâ”€â”€ notebooks/           # Jupyter notebooks for experiments
    â”œâ”€â”€ models/              # Saved models & checkpoints
    â”œâ”€â”€ results/             # Metrics, generated poetry, logs
    â”œâ”€â”€ visualizations/      # PNG plots
    â”œâ”€â”€ logs/                # TensorBoard logs
    â”œâ”€â”€ scripts/             # Training & evaluation scripts
    â””â”€â”€ main.py              # Entry point

## ğŸš€ Getting Started

### 1. Create Virtual Environment

    python -m venv urdu-poetry-env
    source urdu-poetry-env/bin/activate  # Linux/Mac
    urdu-poetry-env\Scripts\activate   # Windows

### 2. Install Dependencies

    pip install tensorflow torch pandas numpy datasets scikit-learn matplotlib seaborn tqdm

### 3. Verify GPU

    python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU')"

## ğŸ“Œ Future Work

-   Word-level tokenization for richer semantics\
-   Hyperparameter tuning with Optuna\
-   Fine-tune GPT-based architectures for Urdu\
-   Meter detection & automatic rhyme scoring
