# Urdu Poetry Generation

## Project Overview
This project explores **Urdu poetry text generation** using sequence models. We implement and compare **Simple RNN, LSTM, and Transformer models** with different optimization algorithms (**Adam, RMSprop, SGD**) to generate coherent, contextually relevant, and grammatically accurate Urdu poetry.

The primary goal is to evaluate models both **quantitatively** (loss, perplexity, accuracy) and **qualitatively** (rhyme, meter, human evaluation).

---

## Dataset
- **Source:** [Hugging Face - ReySajju742/Urdu-Poetry-Dataset](https://huggingface.co/datasets/ReySajju742/Urdu-Poetry-Dataset)  
- **Total poems:** 1,323  
- **Content:** Classical Urdu poetry (Ghalib, Iqbal, others)  
- **Format:** Title and content pairs  
- **Size:** 1.38 MB  

**Preprocessing steps:**
1. Extract individual lines from poems  
2. Character-level tokenization using Keras Tokenizer  
3. Vocabulary creation  
4. Sequence generation using n-grams  
5. Padding sequences to uniform length  
6. Train-validation-test split: 80%-10%-10%  

---

## Project Methodology

### Model Architectures
- **Simple RNN** â€“ Baseline model for sequential prediction  
- **LSTM (Long Short-Term Memory)** â€“ Handles longer dependencies  
- **Transformer** â€“ Attention-based sequence modeling  

### Optimization Algorithms
- **Adam**  
- **RMSprop**  
- **SGD with Momentum**  

### Training Configuration
- **Epochs:** 20â€“30 (with early stopping on validation loss)  
- **Batch Size:** 128  
- **Early Stopping:** Patience = 5  
- **Device:** GPU recommended for faster training  

---

## Evaluation

### Quantitative Metrics
- Training & Validation Loss  
- Test Perplexity  
- Accuracy (character prediction)  
- Training time comparison (minutes)  

### Qualitative Metrics
- Rhyme, meter, and flow of generated poetry  
- Human evaluation of coherence and style  

---

## Model Performance

### Perplexity Comparison (Test Set)

| Model | Optimizer | Test Loss | Perplexity | Training Time (mins) |
|-------|-----------|-----------|------------|--------------------|
| RNN | Adam | 1.6948 | 5.445 | 300 |
| RNN | RMSprop | 1.703 | 5.491 | 300 |
| RNN | SGD | 1.6898 | 5.418 | 300 |
| RNN 2 Layers Dropout 0.2 | Adam | 1.6798 | 5.36 | 1200 |
| LSTM | Adam | 1.5854 | 4.881 | 2400 |
| LSTM | RMSprop | 1.6406 | 5.158 | 1800 |
| LSTM | SGD | 1.5899 | 4.904 | 3420 |
| LSTM 3 Layers | Adam | 1.5563 | 4.742 | 4800 |
| Transformer Set1 | Adam | 1.7194 | 5.581 | 6000 |
| Transformer Set2 | Adam | 1.6902 | 5.420 | 10800 |

> Key findings:  
> - LSTM consistently outperformed RNN and Transformer in perplexity and accuracy.  
> - Transformers took the longest to train but achieved competitive perplexity on smaller architectures.  
> - SGD often resulted in slightly worse performance for both RNN and Transformer.  

---

### Visualizations
- **Perplexity Comparison Plot**  
- **Training Time Comparison Plot**  
- **Perplexity Heatmap by Model and Optimizer**

All plots are saved in the `visualizations/` folder as PNG files. These clearly show the trade-off between performance and training time across models and optimizers.

---

## Sample Generated Poetry

**Seed:** `"Ø¯Ù„ Ú©ÛŒ Ø¨Ø§Øª "`  

**RNN (Adam):**  
Ø¯Ù„ Ú©ÛŒ Ø¨Ø§Øª Ú†Ù„ Ù„ÛŒØ§ Ø³ÛŒ Ù¾Ú¾Ø± Ø¨Ø§Ù‚ÛŒ Ú©ÛŒ Ú©Ø§Ø´ Ù…ÛŒÚº Ú©ÙˆØ¦ÛŒ ÛŒÛ Ø±ÙˆØ­ ÛŒØ§ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø³ÛŒ Ø³Ù† Ø¢Ù…Ø¯ Ú©ÛŒØ§ Ú†ÛŒØ² Ù…ÛŒÚº Ù¾Ú¾Ø± ÛŒÛ ØªÙˆ ØªÙˆ Ù…ÛŒÚº Ù†Û ÛÙˆØ§ Ú©Ø± Ù¾Ú¾Ø± Ø¨Ù†Ø§ Ù„ÛÙˆ Ø¯Ú©Ú¾ Ø¨Ú¾ÛŒ Ø¯Ø¹Ø§ Ø¯Ø§Ù… Ù…ÛŒÚº Ù¾Ø±Ø¯ÛÙ” ØºÙÙ„Û Ùˆ Ø³ÙˆØ§ Ø¯Ù„ Ø³Û’ Ù…Ø±Ø­Ù…Ù†Ø¯Û‚ Ø±Ú©Ú¾ØªØ§ Ø§Ø³ Ø´ÛŒØ´Û Ø³Û’ Ø¨Û’ Ø­Ø§Øª Ø¨Ú¾Ù„Ø§ Ú¯ÛŒØ§ Ø¨Ú¾

**LSTM (Adam):**  
Ù…Ø­Ø¨Øª Ù†ÛÛŒÚº ÛÙˆØ¦ÛŒ Ú©Û Ø¢Ø¦ÛŒ ÛÛ’ Ù…ÛØ±Ø¨Ø§Úº Ú©ÛŒÙˆÚº ÛÛ’ Ú©Û Ø§Ø³
Ú©Ùˆ Ù…Ù„Ù†Ø§ Ù†Û ÛŒÛ ØªÙˆ Ù…Ø±Û’ Ø¨Ø¹Ø¯ Ù…Ù†ØµÙÛŒ Ø³Û’ ÛÙ… Ù†Û’ ØªÙˆ Ø§Ù†
ØªÛŒØ² ÛÛ’ Ø§Ù†ØªØ¸Ø§Ø± Ù†ÛÛŒÚº ÛÙˆØªÛŒ Ø§Ù† Ú©ÛŒ Ø¢Ù†Ú©Ú¾ÙˆÚº Ù…ÛŒÚº Ú©Ú†Ú¾
Ú©ÙˆØ¦ÛŒ Ú©ÛØ§Ù†ÛŒ ØªÚ¾ÛŒ ÙˆÛ Ø¨Ú¾ÛŒ Ø³Ù…Ø¬Ú¾ØªÛ’ ØªÚ¾Û’ Ù¾Ú¾Ø± ØªÙˆ Ø¯ÛŒÚ©Ú¾Ùˆ
ØªÙˆ ØºØ§Ù„Ø¨Ø” Ù…ÛŒÚº Ú©Ø³ÛŒ Ø³Û’ Ú©

**Transformer (Adam):**  
Ø¯Ù„ Ú©ÛŒ Ø¨Ø§Øª ÙˆÛ Ø³Ø¨ Ú©Ú†Ú¾ Ú©ÛØªØ§ ÛÛ’ØŒ Ù„ÛŒÚ©Ù† ÙˆÙ‚Øª Ú©ÛŒ Ø±ÛÙ†Ù…Ø§Ø¦ÛŒ Ù…ÛŒÚº ÛÙ… Ú©ÛÛŒÚº Ú©Ú¾Ùˆ Ø¬Ø§ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± ÛØ± Ù„Ù…Ø­Û Ø¬Ùˆ Ú¯Ø²Ø±ØªØ§ ÛÛ’ØŒ Ù†Ø¦Û’ Ø§Ø´Ø¹Ø§Ø± Ú©ÛŒ Ø´Ú©Ù„ Ø§Ø®ØªÛŒØ§Ø± Ú©Ø± Ù„ÛŒØªØ§ ÛÛ’  

> Note: LSTM generated the most coherent and stylistically consistent sequences.  

---

## ðŸ“‚ Directory Structure

```text
PROJECT - URDU POETRY GENERATION/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ raw_dataset.csv          # Original dataset from Hugging Face
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ cleaned_poetry.csv       # Cleaned text data
â”‚       â”œâ”€â”€ tokenizer.pkl            # Saved Keras tokenizer
â”‚       â””â”€â”€ [train/val/test].npy     # Processed numpy arrays for training
â”‚
â”œâ”€â”€ models/                          # Saved models, training history, and samples
â”‚   â”œâ”€â”€ LSTM/
â”‚   â”œâ”€â”€ RNN/
â”‚   â””â”€â”€ Transformer/
â”‚       # Each subfolder contains:
â”‚       # - Generated text samples (.txt)
â”‚       # - Training history (.npy)
â”‚       # - Saved Model weights (.pt)
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ metrics/
â”‚       â”œâ”€â”€ new_metrics.csv          # Combined performance metrics for all models
â”‚       â””â”€â”€ [Model_Folders]/         # Specific loss/accuracy plots per model
â”‚
â”œâ”€â”€ scripts/                         # Source code
â”‚   â”œâ”€â”€ 01_load_explore_data.py      # EDA and data loading
â”‚   â”œâ”€â”€ 02_preprocess_data.py        # Tokenization and sequence padding
â”‚   â”œâ”€â”€ 03_train_lstm.py             # Training script for LSTM
â”‚   â”œâ”€â”€ 03_train_rnn.py              # Training script for RNN
â”‚   â”œâ”€â”€ 03_train_transformer.py      # Training script for Transformer
â”‚   â”œâ”€â”€ generate_textfiles.py        # Script to generate poetry samples
â”‚   â”œâ”€â”€ utils.py                     # Helper functions
â”‚   â””â”€â”€ visualization.py             # Plotting functions
â”‚
â”œâ”€â”€ visualizations/                  # Final Comparative Analysis Plots
â”‚   â”œâ”€â”€ perplexity_bar_plot.png
â”‚   â”œâ”€â”€ training_time_bar_plot.png
â”‚   â””â”€â”€ perplexity_heatmap.png
â”‚
â”œâ”€â”€ requirements.txt                 # Project dependencies
â””â”€â”€ README.md
```

## Getting Started

### 1. Setup Virtual Environment
```bash
python -m venv urdu-poetry-env
# Activate environment
# Linux/Mac
source urdu-poetry-env/bin/activate
# Windows
urdu-poetry-env\Scripts\activate
```
### 2. Install Dependencies
```bash
pip install tensorflow torch pandas numpy datasets scikit-learn matplotlib seaborn tqdm
```
### 3. Verify GPU
``` bash
python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU')"

```
Notes

All experiments are fully reproducible.

GPU acceleration is strongly recommended for LSTM and Transformer training.

Each model-optimizer combination is saved, logged, and compared systematically.

Both quantitative metrics and qualitative generated poetry are included in the results.